# -*- coding: utf-8 -*-
"""hipotese_init_series.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yt-vDvtQZYI48jYLb0D5CrkDkYLh50qP
"""

# requisitos: pip install neuralforecast[xarray] statsforecast xarray netCDF4 pandas numpy scipy
# (adicione torch, pytorch-lightning se neuralforecast não instalar automaticamente)
import os
import numpy as np
import pandas as pd
import glob as glob
import xarray as xr
from datetime import timedelta
# NeuralForecast
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATS, NHITS, LSTM  # escolha os modelos que quiser
# eventualmente: from neuralforecast.models import TFT, PatchTST, DeepAR

# --------------------------
# Funções de métricas hidrológicas
# --------------------------
def rmse(y_true, y_pred):
    return np.sqrt(np.nanmean((y_true - y_pred) ** 2))

def mae(y_true, y_pred):
    return np.nanmean(np.abs(y_true - y_pred))

def nse(y_true, y_pred):
    # Nash-Sutcliffe Efficiency
    denom = np.nanmean((y_true - np.nanmean(y_true))**2)
    if denom == 0:
        return np.nan
    return 1.0 - (np.nanmean((y_true - y_pred)**2) / denom)

def kge(y_true, y_pred):
    # Kling-Gupta Efficiency (original formulation)
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    # remove nan aligned
    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)
    if mask.sum() == 0:
        return np.nan
    y_true = y_true[mask]; y_pred = y_pred[mask]
    r = np.corrcoef(y_true, y_pred)[0,1]
    if np.isnan(r):
        return np.nan
    alpha = np.std(y_pred)/np.std(y_true) if np.std(y_true) != 0 else np.nan
    beta = np.mean(y_pred)/np.mean(y_true) if np.mean(y_true) != 0 else np.nan
    if np.isnan(alpha) or np.isnan(beta):
        return np.nan
    return 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)

# --------------------------
# EXEMPLO: carregar um NetCDF do CAMELSH para uma estação
# --------------------------
# Pasta onde estão seus arquivos .nc (ex: todos os arquivos de estações)
nc_folder = '/caminho/para/CAMELSH/timeseries/'
pattern = os.path.join(nc_folder, '*.nc')

# Use open_mfdataset para abrir e concatenar todos os arquivos automaticamente
# concat_dim='time' assume que o eixo de interesse é o tempo
ds_all = xr.open_mfdataset(pattern,
                           combine='by_coords',
                           concat_dim='time',
                           parallel=True)  # precisa dask instalado, mas acelera pra muitos arquivos

print(ds_all)

# visualizar variáveis disponíveis:
print(ds_nc)

# normalmente a variável de interesse é 'Streamflow' e as forçantes têm nomes como 'precip' 'air_temp' etc.
# converter para pandas Series
time_index = pd.to_datetime(ds_nc['time'].values)  # ou 'datetime' dependendo do netcdf
q = ds_nc['Streamflow'].to_series()  # pode precisar ajustar nome da variável
# Exemplo: outras covariáveis
if 'Precipitation' in ds_nc.variables:
    precip = ds_nc['Precipitation'].to_series()
else:
    # ajuste para o nome real do arquivo
    precip = None

# montar DataFrame no formato esperado por NeuralForecast:
# columns: unique_id, ds, y  (além de colunas exógenas: exog1, exog2)
df_list = []
uniq_id = 'gage_01234567'
# monta o frame
df_tmp = pd.DataFrame({'ds': time_index, 'y': q.values})
df_tmp['unique_id'] = uniq_id
# adicionar covariáveis passadas (exemplo: precip lagged)
if precip is not None:
    df_tmp['precip'] = precip.values

# neuralforecast prefere df ordenado
df_tmp = df_tmp.sort_values(['unique_id','ds'])
df = df_tmp[['unique_id','ds','y'] + ([c for c in df_tmp.columns if c not in ['unique_id','ds','y']])]

# --------------------------
# Definir modelos e instanciar NeuralForecast
# --------------------------
H = 24  # horizon em horas (exemplo)
freq = 'H'

models = [
    NBEATS(input_size=168, h=H, max_steps=200),   # input_size=história em horas
    NHITS(input_size=168, h=H, max_steps=200),
    LSTM(input_size=168, h=H, max_steps=200)
]

nf = NeuralForecast(models=models, freq=freq)

# --------------------------
# Cross-validation rolling (ex.: 3 janelas)
# --------------------------
# val_size (tamanho da validação em horas) - ex.: 7 dias = 168 horas
val_size = 168
# O método cross_validation do NeuralForecast ajusta modelos para múltiplas janelas e retorna previsões
cv_results = nf.cross_validation(df=df, n_windows=3, step_size=168, test_size=val_size)

# cv_results: DataFrame com colunas ds, unique_id, model1, model2, ... contendo previsões
print(cv_results.head())

# --------------------------
# Avaliação: alinhar previsões com y verdadeiro e calcular métricas por modelo
# --------------------------
# Exemplo para um único modelo:
models_names = [m.__class__.__name__ for m in models]  # names
metrics_per_model = {name: {'RMSE':[], 'MAE':[], 'NSE':[], 'KGE':[]} for name in models_names}

# cv_results tem previsões; precisar recuperar y verdadeiro para os ds correspondentes
for model_name in models_names:
    # join y real
    merged = (cv_results[['unique_id','ds', model_name]]
              .merge(df[['unique_id','ds','y']], on=['unique_id','ds'], how='left'))
    y_true = merged['y'].values
    y_pred = merged[model_name].values
    # calcular métricas
    metrics_per_model[model_name]['RMSE'].append(rmse(y_true, y_pred))
    metrics_per_model[model_name]['MAE'].append(mae(y_true, y_pred))
    metrics_per_model[model_name]['NSE'].append(nse(y_true, y_pred))
    metrics_per_model[model_name]['KGE'].append(kge(y_true, y_pred))

# Resumo final
for mname, mets in metrics_per_model.items():
    print(f"Model {mname}:")
    for k,v in mets.items():
        print(f"  {k}: mean over windows = {np.nanmean(v):.4f}")

# --------------------------
# Observações finais do script:
# - para rodar em múltiplos gauges, faça loop sobre arquivos NetCDF, construa um df concatenado com multiple unique_id
# - Ajuste hyperparams: max_steps, batch_size, input_size, learning_rate
# - Para saídas probabilísticas (DeepAR, etc.), use colunas probabilísticas retornadas e calcule CRPS (biblioteca properscoring ajuda)