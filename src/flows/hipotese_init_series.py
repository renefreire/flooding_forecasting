# -*- coding: utf-8 -*-
"""
hipotese_init_series.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yt-vDvtQZYI48jYLb0D5CrkDkYLh50qP

requisitos: pip install -r requirements.txt
"""

# ==========================
# Imports
# ==========================
import sys
import os
import numpy as np
import pandas as pd
import xarray as xr
import glob
import dask

# NeuralForecast
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATS, NHITS, LSTM  # escolha os modelos que quiser
# eventualmente: from neuralforecast.models import TFT, PatchTST, DeepAR

# Caminho absoluto para o diretório 'src'
src_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'src'))
sys.path.insert(0, src_path)

# ==========================
# Métricas hidrológicas
# ==========================
def rmse(y_true, y_pred):
    return np.sqrt(np.nanmean((y_true - y_pred) ** 2))

def mae(y_true, y_pred):
    return np.nanmean(np.abs(y_true - y_pred))

def nse(y_true, y_pred):
    # Nash-Sutcliffe Efficiency
    denom = np.nanmean((y_true - np.nanmean(y_true))**2)
    if denom == 0:
        return np.nan
    return 1.0 - (np.nanmean((y_true - y_pred)**2) / denom)

def kge(y_true, y_pred):
    # Kling-Gupta Efficiency (original formulation)
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    # remove nan aligned
    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)
    if mask.sum() == 0:
        return np.nan
    y_true = y_true[mask]; y_pred = y_pred[mask]
    r = np.corrcoef(y_true, y_pred)[0,1]
    if np.isnan(r):
        return np.nan
    alpha = np.std(y_pred)/np.std(y_true) if np.std(y_true) != 0 else np.nan
    beta = np.mean(y_pred)/np.mean(y_true) if np.mean(y_true) != 0 else np.nan
    if np.isnan(alpha) or np.isnan(beta):
        return np.nan
    return 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)

# ==========================
# Leitura dos dados NetCDF
# ==========================
# Pasta onde estão seus arquivos .nc (ex: todos os arquivos de estações)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
nc_folder = os.path.join(BASE_DIR, '..', 'dataset')
nc_folder = os.path.abspath(nc_folder)
pattern = os.path.join(nc_folder, '*.nc')

# Debug seguro
print("Pasta dataset:", nc_folder)
#print("Arquivos encontrados:", glob.glob(pattern))
nc_files = glob.glob(pattern)
if not nc_files:
    raise FileNotFoundError(f"Nenhum arquivo .nc encontrado em {nc_folder}")

# Use open_mfdataset para abrir e concatenar todos os arquivos automaticamente
# concat_dim='time' assume que o eixo de interesse é o tempo
ds_all = xr.open_mfdataset(pattern,
                           combine='by_coords',
                           #concat_dim='time',
                           parallel=True)  # precisa dask instalado, mas acelera pra muitos arquivos

print(ds_all)

# -------------------------------------------------
# Selecionar UMA estação (exemplo)
# -------------------------------------------------
# Aqui assumimos que cada arquivo é uma estação
nc_files = sorted([os.path.join(nc_folder, f) for f in os.listdir(nc_folder) if f.endswith('.nc')])

# Selecionamos o primeiro arquivo explicitamente
ds_nc = xr.open_dataset(nc_files[0])

# ==========================
# Extração das variáveis
# ==========================
# Ajuste os nomes conforme o NetCDF real
time_index = pd.to_datetime(ds_nc['DateTime'].values)

# Variável alvo: vazão
q = ds_nc['Streamflow'].to_series().values

# Covariável opcional
precip = None
if 'Precipitation' in ds_nc.variables:
    precip = ds_nc['Precipitation'].to_series().values

# ==========================
# Montagem do DataFrame (NeuralForecast)
# ==========================
unique_id = 'gage_001'

df = pd.DataFrame({
    'unique_id': unique_id,
    'ds': time_index,
    'y': q
})

if precip is not None:
    df['precip'] = precip

# Remover possíveis NaNs iniciais
df = df.dropna(subset=['y'])

# Ordenação obrigatória
df = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)

print(df.head())

# ==========================
# Definir modelos e instanciar NeuralForecast
# ==========================
H = 24  # horizon em horas (exemplo)
INPUT_SIZE = 168
MAX_STEPS = 200
freq = 'H'

models = [
    NBEATS(input_size=INPUT_SIZE, h=H, max_steps=MAX_STEPS),   # input_size=história em horas
    NHITS(input_size=INPUT_SIZE, h=H, max_steps=MAX_STEPS),
    LSTM(input_size=INPUT_SIZE, h=H, max_steps=MAX_STEPS)
]

nf = NeuralForecast(models=models, freq=freq)

# ==========================
# Validação cruzada temporal
# ==========================
val_size = 168

cv_results = nf.cross_validation(
    df=df,
    n_windows=3,
    step_size=168
)

print(cv_results.head())

# ==========================
# Avaliação
# ==========================
model_names = [m.__class__.__name__ for m in models]

metrics = {m: {'RMSE': [], 'MAE': [], 'NSE': [], 'KGE': []} for m in model_names}

for model_name in model_names:
    merged = cv_results[['unique_id', 'ds', model_name]].merge(
        df[['unique_id', 'ds', 'y']],
        on=['unique_id', 'ds'],
        how='left'
    )

    y_true = merged['y'].values
    y_pred = merged[model_name].values

    metrics[model_name]['RMSE'].append(rmse(y_true, y_pred))
    metrics[model_name]['MAE'].append(mae(y_true, y_pred))
    metrics[model_name]['NSE'].append(nse(y_true, y_pred))
    metrics[model_name]['KGE'].append(kge(y_true, y_pred))

# ==========================
# Resultados finais
# ==========================
for model, mets in metrics.items():
    print(f'\nModel: {model}')
    for k, v in mets.items():
        print(f'  {k}: {np.nanmean(v):.4f}')

# --------------------------
# Observações finais do script:
# - para rodar em múltiplos gauges, faça loop sobre arquivos NetCDF, construa um df concatenado com 
#   multiple unique_id
# - Ajuste hyperparams: max_steps, batch_size, input_size, learning_rate
# - Para saídas probabilísticas (DeepAR, etc.), use colunas probabilísticas retornadas e calcule CRPS 
#   (biblioteca properscoring ajuda)